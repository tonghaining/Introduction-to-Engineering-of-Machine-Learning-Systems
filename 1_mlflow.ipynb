{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Exercise Background\n",
    "\n",
    "<img src=\"images/overview-mlflow-focus.jpg\" width=800>\n",
    "\n",
    "As shown in the figure above, the MLOps platform provides an MLflow service that stores ML model training information to a PostgreSQL database and training artifacts to a MinIO storage service. \n",
    "\n",
    "MLflow provides a [Python client](https://mlflow.org/docs/latest/python_api/index.html) for communicating with an MLflow service. For example, we can use the MLflow Python client to start an *MLflow run*, which is an execution of an ML training script:\n",
    "```python\n",
    "with mlflow.start_run():\n",
    "    model = ElasticNet(alpha=..., l1_ratio=...)\n",
    "    model.fit(train_x, train_y)\n",
    "```\n",
    "*MLflow runs* are organized into *MLflow experiments*. An MLflow experiment can be seen as a logical unit of one or more MLflow runs. For example, there can be an MLflow experiment for training an ElasticNet model, and there can be multiple MLflow runs under this experiment for exploring different hyperparameters and/or training datasets.\n",
    "\n",
    "When starting  an MLflow run, we can record the relevant information, such as the configured hyperparameters and custom evaluation metrics. After the run is completed, we can also upload the produced model artifact to MLflow:\n",
    "```python\n",
    "with mlflow.start_run():\n",
    "    model = ElasticNet(alpha=..., l1_ratio=...)\n",
    "    model.fit(train_x, train_y)\n",
    "    mlflow.log_param(\"alpha\", ...)\n",
    "    mlflow.log_param(\"l1_ratio\", ...)\n",
    "    mlflow.log_metric(\"rmse\", ...)\n",
    "    mlflow.sklearn.log_model(model, ...)\n",
    "```\n",
    "Below is a complete example. \n",
    "\n",
    "More reading material: [MLflow docs](https://mlflow.org/docs/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Connection to MLflow Server\n",
    "Before starting the exercises, please ensure that you can connect to the MLflow server. Please use the mlops_eng environment which has setup the MLflow client for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def test_connection():\n",
    "    for url in [\n",
    "        \"kserve-gateway.local\",\n",
    "        \"ml-pipeline-ui.local\",\n",
    "        \"mlflow-server.local\",\n",
    "        \"mlflow-minio-ui.local\",\n",
    "        \"mlflow-minio.local\",\n",
    "        \"prometheus-server.local\",\n",
    "        \"grafana-server.local\",\n",
    "        \"evidently-monitor-ui.local\",\n",
    "    ]:\n",
    "        try:\n",
    "            requests.get(f\"http://{url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {url}: {e}\")\n",
    "            raise e\n",
    "\n",
    "test_connection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take a look at the dataset\n",
    "\n",
    "In this example, we'll use sklearn to train a simple ElasticNet model that predicts red wine quality given some chemical attributes. The information of dataset used in this example can be found [here](https://archive.ics.uci.edu/dataset/186/wine+quality).\n",
    "\n",
    "Deepchecks also provides convenient access to this dataset:\n",
    "\n",
    "```python\n",
    "\n",
    "from deepchecks.tabular.datasets.regression import wine_quality\n",
    "train_data, test_data = wine_quality.load_data(as_train_test=True)\n",
    "```\n",
    "\n",
    "Below, we load the prepared training and test CSV files and inspect the first 10 rows of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv('data/1_data_train.csv')\n",
    "test_data  = pd.read_csv('data/1_data_test.csv')\n",
    "target = 'quality'\n",
    "\n",
    "# TRAIN\n",
    "y_train = train_data[target]\n",
    "X_train = train_data.drop(columns=[target])\n",
    "\n",
    "# TEST\n",
    "y_test = test_data[target]\n",
    "X_test = test_data.drop(columns=[target])\n",
    "\n",
    "train_ds = Dataset(X_train, label=y_train, cat_features=[])\n",
    "test_ds  = Dataset(X_test,  label=y_test,  cat_features=[])\n",
    "\n",
    "X_train.nlargest(10, 'alcohol') # Get top 10 rows with largest alcohol values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an MLflow run\n",
    "\n",
    "Now, let's use the loaded dataset to create an MLflow run that trains an simple model and logs relevant parameters, metrics, and artifacts to the MLflow service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Setup\n",
    "The following code snippet exemplifies how to use the MLflow Python client to record training parameters and evaluation metrics as well as upload the trained model artifact to the MLflow service.\n",
    "\n",
    "The following code snippet import the necessary Python packages and configure the environment for MLflow.\n",
    "Do not modify this cell â€“ it sets up logging, MLflow endpoints, and access credentials needed for the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Set an environmental variable named \"MLFLOW_S3_ENDPOINT_URL\" so that MLflow client knows where to save artifacts.\n",
    "# The MinIO storage service can be accessed via http://mlflow-minio.local\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://mlflow-minio.local\"\n",
    "\n",
    "# Configure the credentials needed for accessing the MinIO storage service.\n",
    "# \"AWS_ACCESS_KEY_ID\" has been configured in a ComfigMap and \"AWS_SECRET_ACCESS_KEY\" in a Secret in your K8s cluster when you set up the MLOps platform\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"minioadmin\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"minioadmin\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Define MLflow experiment function\n",
    "Instead of hardcoding a model, experiment, or model name, we will define a function run_mlflow_experiment(...) that:\n",
    "\n",
    "- Trains a given model on the training dataset\n",
    "\n",
    "- Evaluates it on the test dataset\n",
    "\n",
    "- Logs parameters, metrics, and the trained model to MLflow\n",
    "\n",
    "This allows you to easily experiment with different models and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def eval_metrics(actual, pred):\n",
    "    rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "    return rmse\n",
    "\n",
    "def run_mlflow_experiment(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    model,\n",
    "    experiment_name=\"default_experiment\",\n",
    "    model_name=\"my_model\",\n",
    "    tracking_uri=\"http://mlflow-server.local\",\n",
    "    params=None\n",
    "):\n",
    "    \"\"\"Run a single MLflow experiment with a given model and log metrics + model.\"\"\"\n",
    "    \n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    with mlflow.start_run() as run:\n",
    "        print(\"MLflow run_id:\", run.info.run_id)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        # Compute metric\n",
    "        rmse = eval_metrics(y_test, predictions)\n",
    "        \n",
    "        # Log params\n",
    "        if params:\n",
    "            for k, v in params.items():\n",
    "                mlflow.log_param(k, v)\n",
    "                \n",
    "        # Log metric\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(model, artifact_path=\"model\", registered_model_name=model_name)\n",
    "        print(\"Logged model artifact URI:\", mlflow.get_artifact_uri(\"model\"))\n",
    "        \n",
    "        return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Run a sample model training experiment\n",
    "\n",
    "We will run a simple ElasticNet model with default hyperparameters.\n",
    "\n",
    "- This will serve as a baseline for comparison.\n",
    "\n",
    "- The run will be recorded in MLflow so you can view the metrics and model artifact in the web UI.\n",
    "\n",
    "- Before running, set a meaningful experiment name and model name, as these will be visible to others on the MLflow server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "default_model = ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42)\n",
    "default_params = {\"alpha\": 0.5, \"l1_ratio\": 0.5}\n",
    "# MLFLOW_EXPERIMENT_NAME = \"YOUR_EXPERIMENT_NAME_HERE\" # Remember that others on the same MLflow service may see these names so be nice.\n",
    "# MODEL_NAME=\"YOUR_MODEL_NAME\"\n",
    "\n",
    "rmse = run_mlflow_experiment(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    model=default_model,\n",
    "    experiment_name=MLFLOW_EXPERIMENT_NAME,\n",
    "    model_name=MODEL_NAME,\n",
    "    params=default_params\n",
    ")\n",
    "print(\"RMSE for default ElasticNet:\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```text\n",
    "2025/11/30 13:10:56 INFO mlflow.tracking.fluent: Experiment with name 'wine_quality_experiment' does not exist. Creating a new experiment.\n",
    "MLflow run_id: ccc5efce623f4cf69d3fb35d698221af\n",
    "INFO:botocore.credentials:Found credentials in environment variables.\n",
    "Successfully registered model 'elasticnet_model'.\n",
    "2025/11/30 13:11:00 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: elasticnet_model, version 1\n",
    "Logged model artifact URI: s3://mlflow/3/ccc5efce623f4cf69d3fb35d698221af/artifacts/model\n",
    "RMSE for default ElasticNet: 0.6777347143112558\n",
    "Created version '1' of model 'elasticnet_model'.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to the MLflow service UI at [http:/127.0.0.1:5000/](http:/127.0.0.1:5000/) (ported from the http://mlflow-server.local URL inside the cluster),\n",
    "and you should see your run under the experiment \"mlflow-minio-test\". You can browse the run parameters, metrics and artifacts. For example: \n",
    "\n",
    "* Training hyperparameters and evaluation metrics:\n",
    "\n",
    "<img src=\"images/mlflow-logging.png\" width=\"1000\"/>\n",
    "\n",
    "You may notice that the \"Metrics\" and \"Parameters\" field are hidden by default, you can make them visible by clicking the \"Columns\" tab:\n",
    "\n",
    "<img src=\"images/mlflow-show-columns.png\" width=1000 />\n",
    "\n",
    "When clicking the Run Name, we can also check where the model and other related files have been uploaded:\n",
    "\n",
    "<img src=\"images/mlflow-uploaded-artifacts.png\" width=1000 />\n",
    "\n",
    "In this case, the model (which is a Pickle file) and its related files (such as the model dependency requirements) have been uploaded to the MinIO service. Navigate to [http:/127.0.0.1:9000/](http:/127.0.0.1:9000/) (ported from the http://mlflow-minio.local inside the cluster) and login using \"minioadmin\" as both the username and password, we can see there is a bucket named \"mlflow\":\n",
    "\n",
    "<img src=\"images/minio-bucket-ui.png\" width=1000 />\n",
    "\n",
    "clicking the bucket (and its underlying folders) we can see the model and its related artifacts reside in the \"mlflow\" bucket:\n",
    "\n",
    "<img src=\"images/minio-model-artifacts.png\" width=1000 />\n",
    "\n",
    "* Finally, we can also see the model has been registered to MLflow:\n",
    "\n",
    "<img src=\"images/mlflow-model.png\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Try different hyperparameters\n",
    "\n",
    "Try running the run_mlflow_experiment(...) function with different hyperparameters for the ElasticNet model.\n",
    "MLflow will record each run separately, allowing you to compare results easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [] # Add your alpha values here\n",
    "l1_ratios = [] # Add your l1_ratio values here\n",
    "\n",
    "for alpha in alphas:\n",
    "    for l1_ratio in l1_ratios:\n",
    "        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n",
    "        run_mlflow_experiment(\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            model=model,\n",
    "            experiment_name=MLFLOW_EXPERIMENT_NAME,\n",
    "            model_name=MODEL_NAME,\n",
    "            params={\"alpha\": alpha, \"l1_ratio\": l1_ratio}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Try different models\n",
    "Now you can experiment with a different model, such as a RandomForestRegressor from sklearn (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) which is shown below. The goal is to see how the RandomForest model performs compared to the ElasticNet model, and log the results to MLflow as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_params = {\"n_estimators\": 100, \"max_depth\": 5}\n",
    "#MLFLOW_RF_EXPERIMENT_NAME = \"YOUR_RF_EXPERIMENT_NAME\"\n",
    "#MODEL_RF_NAME=\"YOUR_RF_MODEL_NAME\"\n",
    "\n",
    "rmse_rf = run_mlflow_experiment(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    model=rf_model,\n",
    "    experiment_name=MLFLOW_RF_EXPERIMENT_NAME,\n",
    "    model_name=MODEL_RF_NAME,\n",
    "    params=rf_params\n",
    ")\n",
    "print(\"RMSE for RandomForest:\", rmse_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please add code cells below to try different models and hyperparameters, the goal is to get the best possible RMSE on the test dataset while logging all runs to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different models and hyperparameters here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Explore the MLflow UI and Inspect Team Runs\n",
    "\n",
    "- Use the MLflow web UI or CLI to inspect the runs, metrics, parameters, and logged model artifacts.\n",
    "\n",
    "- Filter by experiment name or model name to see only your runs.\n",
    "\n",
    "- Try loading a saved model from MLflow and make predictions on new samples.\n",
    "\n",
    "Can you identify which model and hyperparameter combination achieved the best RMSE on the test dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
