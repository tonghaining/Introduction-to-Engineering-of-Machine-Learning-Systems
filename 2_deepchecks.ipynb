{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efade87d-5f6a-4645-8feb-5d1f3bf26abd",
   "metadata": {},
   "source": [
    " # Deepchecks tutorial (tabular)\n",
    "\n",
    " This notebook shows a realistic “pre-deployment validation” workflow.\n",
    "\n",
    " We start from a model already trained and logged to MLflow in the previous tutorial. We then validate that it behaves as expected on the familiar `1_*` test split. After that, we introduce a new batch `2_*` where the data distribution is different in a way that was not covered in `1_*`. Deepchecks will help you detect what changed and which slices are problematic.\n",
    "\n",
    " The datasets are intentionally constructed so that `1_*` does not cover an extreme slice of the feature space (for pedagogical reasons), while `2_*` re-introduces that slice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaginary scenario\n",
    "Perhaps you are a large-scale wine producer, and for each batch of wine you produce, a tasting team evaluates a small sample of the wines for quality control (test set), and you want to check does your prediction model trained on historical data (training set) agree with them. For example, maybe for the newest batch, your model does not agree with the experts on the samples (wines) especially on one input column, making you suspect something is wrong.\n",
    "\n",
    "Deepchecks can help you develop an automated test for this, and keep track which features your model has the most problems with. \n",
    "\n",
    "In practice, the first question is ‘what changed in the data?’—so we start with drift checks before looking at performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d674c6b-5a36-4124-8c3d-2e5e129b5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, ensure your working directory is the repository root.\n",
    "# (If it is correct already, running this cell is harmless.)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da57e82-00be-4604-aa37-1d497f205c04",
   "metadata": {},
   "source": [
    " ## 1) Load a model from MLflow\n",
    "\n",
    " Open the MLflow UI, locate your best run from the previous tutorial, and copy its `run_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e733b01-c983-4f2b-996b-322b48ca120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.misc import load_model\n",
    "#RUN_ID = \"ADD YOUR RUN ID HERE FROM PREVIOUS EXERCISE\"\n",
    "model = load_model(RUN_ID)\n",
    "print(\"Loaded model:\", type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc6824-f48d-41aa-8556-b2c3c4d69c62",
   "metadata": {},
   "source": [
    " ## 2) Load data\n",
    "\n",
    " We will use `1_data_train.csv` as the reference distribution (what the model has effectively seen), and we will compare two different test batches against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f750f-1787-4e1b-8f6c-ad9533af3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deepchecks.tabular import Dataset\n",
    "\n",
    "train_v1 = pd.read_csv('data/1_data_train.csv')\n",
    "test_v1  = pd.read_csv('data/1_data_test.csv')\n",
    "test_v2  = pd.read_csv('data/2_data_test.csv')\n",
    "\n",
    "train_ds_v1 = Dataset(train_v1, label='quality', cat_features=[])\n",
    "test_ds_v1  = Dataset(test_v1,  label='quality', cat_features=[])\n",
    "test_ds_v2  = Dataset(test_v2,  label='quality', cat_features=[])\n",
    "\n",
    "print(f\"train_v1: {train_v1.shape}\")\n",
    "print(f\"test_v1:  {test_v1.shape}\")\n",
    "print(f\"test_v2:  {test_v2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56f9ec-9532-4bb1-ba56-2aa3de27b11e",
   "metadata": {},
   "source": [
    " ## 3) Define a small validation suite\n",
    "\n",
    " We use two automated checks that should pass on the familiar `1_*` test split and fail on the new `2_*` batch.\n",
    "\n",
    " Feature drift compares the per-feature distributions between reference (train) and the candidate batch (test).\n",
    "\n",
    " Prediction drift checks whether the model’s prediction distribution on the candidate batch differs from what it produces on the reference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711b6d11-adc2-4332-be92-de15c28fc567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular import Suite\n",
    "from deepchecks.tabular.checks import FeatureDrift, PredictionDrift\n",
    "\n",
    "validation_suite = Suite(\n",
    "    \"Wine model: batch validation\",\n",
    "    FeatureDrift(sort_feature_by='drift score', n_top_columns=10)\n",
    "        .add_condition_drift_score_less_than(\n",
    "            max_allowed_numeric_score=0.2,\n",
    "            max_allowed_categorical_score=0.2,\n",
    "            allowed_num_features_exceeding_threshold=0,\n",
    "        ),\n",
    "    PredictionDrift()\n",
    "        .add_condition_drift_score_less_than(max_allowed_drift_score=0.2),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b674de28-08de-46b8-8fbc-98d0b07b13ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_suite_result(suite_result):\n",
    "    failed_or_warn = suite_result.get_not_passed_checks(fail_if_warning=True)\n",
    "    failed_only = suite_result.get_not_passed_checks(fail_if_warning=False)\n",
    "    not_ran = suite_result.get_not_ran_checks()\n",
    "\n",
    "    print(\"Failed (FAIL only):\", len(failed_only))\n",
    "    print(\"Failed (FAIL+WARN):\", len(failed_or_warn))\n",
    "    print(\"Not ran:\", len(not_ran))\n",
    "\n",
    "    if len(failed_or_warn) > 0:\n",
    "        print(\"\\nNot-passed checks:\")\n",
    "        for r in failed_or_warn:\n",
    "            # Works for both CheckResult and CheckFailure\n",
    "            try:\n",
    "                name = r.get_metadata().get(\"name\", None)\n",
    "            except Exception:\n",
    "                name = None\n",
    "            if not name:\n",
    "                try:\n",
    "                    name = r.get_header()\n",
    "                except Exception:\n",
    "                    name = type(r).__name__\n",
    "            print(\"-\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1573959e-d44a-491a-bac8-20dd2a74f8da",
   "metadata": {},
   "source": [
    " ## 4) Run checks on the familiar batch (`1_*`)\n",
    "\n",
    " This is your baseline. If this fails, fix the model or your data pipeline before you do any batch-to-batch comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720b5be-cc94-4e7c-9ee1-68032be7dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_v1 = validation_suite.run(train_ds_v1, test_ds_v1, model=model)\n",
    "summarize_suite_result(result_v1)\n",
    "result_v1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f31b0-38e9-4e10-92a1-de89deffe318",
   "metadata": {},
   "source": [
    " ## 5) Run checks on the new batch (`2_*`)\n",
    "\n",
    " This is the deployment-like scenario: the reference distribution is still `train_v1`, but the incoming data is different. The suite should now fail at least on feature drift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc47ac-02d4-4442-b5c6-b526ef890ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_v2 = validation_suite.run(train_ds_v1, test_ds_v2, model=model)\n",
    "summarize_suite_result(result_v2)\n",
    "result_v2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac94b476-cc8b-4b5c-b6e2-e6eec3e58429",
   "metadata": {},
   "source": [
    " ## 6) Optional: quantify the performance change\n",
    "\n",
    " Deepchecks focuses on model and data validation logic. It is still useful to print a simple metric on each test batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0307d-4d86-4e17-96c0-5cec21ffff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "X1 = test_v1.drop(columns=['quality'])\n",
    "y1 = test_v1['quality']\n",
    "X2 = test_v2.drop(columns=['quality'])\n",
    "y2 = test_v2['quality']\n",
    "\n",
    "pred1 = model.predict(X1)\n",
    "pred2 = model.predict(X2)\n",
    "\n",
    "rmse1 = float(np.sqrt(mean_squared_error(y1, pred1)))\n",
    "rmse2 = float(np.sqrt(mean_squared_error(y2, pred2)))\n",
    "mae1 = float(mean_absolute_error(y1, pred1))\n",
    "mae2 = float(mean_absolute_error(y2, pred2))\n",
    "\n",
    "print(f\"test_v1: RMSE={rmse1:.4f}, MAE={mae1:.4f}\")\n",
    "print(f\"test_v2: RMSE={rmse2:.4f}, MAE={mae2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca1e28-e42c-4146-89f7-9e7bbb177ec2",
   "metadata": {},
   "source": [
    " ## 7) Diagnostic: automatically surface weak slices\n",
    "\n",
    " This view is meant to give you a concrete “where is the model struggling?” view, which can be helpful once you see drift. It is not necessary to use this view to complete the task, but it can be interesting perspective in many realistic scenarios\n",
    " \n",
    " *IF* the same variable is automatically chosen here by DeepChecks for comparison (it depends on the model you trained), it should be obvious also in this view, that at a particular slice for a particular (single) variable, the model is poor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b2caf-4578-40f9-98f7-675fe86c43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import WeakSegmentsPerformance\n",
    "\n",
    "weak_segments = WeakSegmentsPerformance(segment_minimum_size_ratio=0.05)\n",
    "weak_v1 = weak_segments.run(test_ds_v1, model=model)\n",
    "weak_v2 = weak_segments.run(test_ds_v2, model=model)\n",
    "\n",
    "print(\"Weak segments on test_v1 (baseline). Probably no clear problems here\")\n",
    "weak_v1.show()\n",
    "\n",
    "print(\"Weak segments on test_v2 (new batch). Look for clear problems here. Remember to check all tabs\")\n",
    "weak_v2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edeadad-d80f-4154-a5ec-2435bcf230cf",
   "metadata": {},
   "source": [
    " ## 8) Student task: identify which column drifted the most\n",
    "\n",
    " In the `Feature Drift` output from the `test_v2` suite run, find the non-label column with the highest drift score.\n",
    " The probability density for the testv2 dataset should have a tail in its distribution that is completely missing from the trainv1 dataset.\n",
    "\n",
    " Copy its name into `suspect_feature` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c109644-5a40-47e4-a5fe-b92644bf35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suspect_feature = \"REPLACE_WITH_THE_SINGLE_FEATURE_NAME_YOU_FOUND\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc5333-a22a-435e-a221-569940227aa1",
   "metadata": {},
   "source": [
    " ## 9) Confirm your hypothesis with a one-feature drift plot\n",
    "\n",
    " This cell should show a drift plot for exactly one feature.\n",
    " If you picked the right feature, the distribution shift should be visually obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3dd28e-0768-49a8-b5c5-55eae810d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import FeatureDrift\n",
    "\n",
    "def show_one_feature_drift(train_ds, test_ds, feature_name, model):\n",
    "    check = FeatureDrift(columns=[feature_name], n_top_columns=1, sort_feature_by='drift score')\n",
    "    out = check.run(train_dataset=train_ds, test_dataset=test_ds, model=model)\n",
    "    out.show(show_additional_outputs=False)\n",
    "    return out\n",
    "\n",
    "show_one_feature_drift(train_ds_v1, test_ds_v2, suspect_feature, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e65f2f43-3431-4b6e-a9f1-db1420d9695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A printed table below can be faster to scan than the tabs, once you know what you are looking for\n",
    "# It lists the segments with the worst performance first.\n",
    "\n",
    "# If you run this, it should be very obvious where the problem was.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def show_feature_drift_scores(suite_result, check_header=\"Feature Drift\", top_k=12):\n",
    "    # Pull the single check result out of the suite by its displayed header\n",
    "    fd_result = suite_result.select_results(names={check_header})[0]  # CheckResult\n",
    "    d = fd_result.value  # dict: feature -> {'Drift score': ..., 'Method': ..., 'Importance': ...}\n",
    "\n",
    "    df = (pd.DataFrame.from_dict(d, orient=\"index\")\n",
    "            .rename_axis(\"feature\")\n",
    "            .reset_index()\n",
    "            .sort_values(\"Drift score\", ascending=False))\n",
    "\n",
    "    display(df.head(top_k))\n",
    "    return df\n",
    "\n",
    "# print(\"Top drifted features on test_v1:\")   # should be small\n",
    "# _ = show_feature_drift_scores(result_v1, top_k=12)\n",
    "\n",
    "print(\"Top drifted features on test_v2:\")   # should clearly surface the missing-slice feature\n",
    "_ = show_feature_drift_scores(result_v2, top_k=12)\n",
    "\"\"\"\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca33e9d-b09c-4c50-9b68-0a084c14c097",
   "metadata": {},
   "source": [
    " ## 10) Retrain on the updated training split (`2_*`)\n",
    "\n",
    " The `2_*` split contains the missing slice. Retraining on it is the simplest “fix” for this exercise.\n",
    "\n",
    " You can simply uncomment the single line that calls `retrain_model`.\n",
    " It is using exactly the same code as the first notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2aab1c1-e107-444b-893e-fb71fd0133b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.retrain_model import retrain_model\n",
    "\n",
    "train_v2 = pd.read_csv('data/2_data_train.csv')\n",
    "\n",
    "X_train = train_v2.drop(columns=['quality'])\n",
    "y_train = train_v2['quality']\n",
    "\n",
    "X_test = test_v2.drop(columns=['quality'])\n",
    "y_test = test_v2['quality']\n",
    "\n",
    "# EXPERIMENT_NAME = \"YOUR EXPERIMENT NAME\" # You can use the same one for all of your models.\n",
    "# MODEL_NAME = \"YOUR MODEL NAME\"\n",
    "\n",
    "# retrain_model(X_train, y_train, X_test, y_test, experiment_name=EXPERIMENT_NAME, model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426bd885-de96-479b-8e80-1d9fd586a4d8",
   "metadata": {},
   "source": [
    " ## 11) Load the retrained model and rerun validation on `2_*`\n",
    "\n",
    " After retraining, MLflow prints a new `run_id`. Paste it below, load the new model, and rerun the same checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d20c5-d95d-4a70-bab4-10a8b728d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW_RUN_ID = \"PASTE_THE_NEW_RUN_ID_HERE\"\n",
    "new_model = load_model(NEW_RUN_ID)\n",
    "print(\"Loaded model:\", type(new_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613031a5-8053-4171-becf-c175bc2c65c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_v2 = Dataset(train_v2, label='quality', cat_features=[])\n",
    "\n",
    "after_retrain = validation_suite.run(train_ds_v2, test_ds_v2, model=new_model)\n",
    "summarize_suite_result(after_retrain)\n",
    "after_retrain.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f8bb0-8a40-4050-9762-418dabe8d9f7",
   "metadata": {},
   "source": [
    " And the single feature drift plot should now look good too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab1f4d4-2d8e-4b7d-8e26-2e20064086b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fixed result for the suspect feature:\")\n",
    "result2 = show_one_feature_drift(train_ds_v2, test_ds_v2, suspect_feature, model=new_model)\n",
    "result2.show()\n",
    "\n",
    "\n",
    "print(\"Old result for the suspect feature:\")\n",
    "result1 = show_one_feature_drift(train_ds_v1, test_ds_v2, suspect_feature, model=model)\n",
    "result1.show()\n",
    "\n",
    "print(\"It should be obvious the latter one is worse!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: the full DeepChecks suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.suites import model_evaluation\n",
    "suite = model_evaluation().run(train_ds_v1, test_ds_v2, model)\n",
    "suite.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
